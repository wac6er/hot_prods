{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "16fNQTpIKeTl"
      },
      "source": [
        "**Notes:** Press run, enter your preferred amazon product, and then enter your preffered # of topics: \n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#import packages\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import requests\n",
        "import time\n",
        "import bs4\n",
        "from bs4 import BeautifulSoup\n",
        "import random\n",
        "import re\n",
        "\n",
        "print('run data collection? (yes/no)')\n",
        "data_collection = input().lower()\n",
        "print('run topic model? (yes/no)')\n",
        "topicc_model = input().lower()\n",
        "print('enter your level (1-10)')\n",
        "level = int(input())\n",
        "print('enter your preferred sleep time (for levels > 10, over 5s is reccommended):')\n",
        "sleep_time = int(input())\n",
        "print('enter your preferred # of topics for the topic model:')\n",
        "xcv = int(input())\n",
        "\n",
        "saver = {'review':[],'title':[],'verified':[],'text':[]}\n",
        "uu = {'text':[]}\n",
        "ccc = {'text':[]}\n",
        "cc = {'text':[]}\n",
        "saverr = {'links':[]}\n",
        "if data_collection =='yes':\n",
        "  level = level*1\n",
        "  print('enter the product type / brand reviews you would like to analyze')\n",
        "  sss = str(input())\n",
        "  for xzx in range(level*1):\n",
        "    url = 'https://www.amazon.com/s?k='+sss+':&page='+str(xzx)\n",
        "    headers = {'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_14_6) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/99.0.4844.83 Safari/537.36'}\n",
        "    cookies = {'session-id':'132-9175929-1227644','session-id-time':'2082787201l','i18n-prefs':'USD','csm-hit':'tb:s-1KBXPV9W0G7AT013EMW9|1648478470844&t:1648478472672&adb:adblk_no','ubid-main':'132-0411939-8328406','session-token':'IG4OEnTNUJl902553Nw+BD3QU3B6FYKyBosq1bxrqVuPzgnyOBCuRYeW9ehTsHsD3gSqEHENInXJhIWOGMUSaPJCKFfUQR1YXX3qWdF+HNoWuh8eijUOpD7dcLRZJpr7WqRdnqaJoA+TxLT7ot7b3my/B9N2foW+7sa0kWmKh3hHZB9iZN3BiPhdUnM6itB3'}\n",
        "    html = requests.get(url,headers = headers, cookies =cookies).text\n",
        "    soup = bs4.BeautifulSoup(html, \"html.parser\")\n",
        "    \n",
        "\n",
        "    for tag in soup.findAll(\"div\", {\"class\": \"s-main-slot s-result-list s-search-results sg-row\"}):\n",
        "      xxxx = tag.findAll(\"a\",{\"class\":\"a-link-normal\"})\n",
        "      if len(xxxx)>=1:\n",
        "        saverr['links'].append(xxxx)\n",
        "      else:\n",
        "        print(xxxx)\n",
        "    len(saverr['links'])\n",
        "\n",
        "    from numpy.ma.core import exp\n",
        "    cust_rev = {'id':[],'name':[],'link':[]}\n",
        "    for xc in range(len(str(saverr['links'][0]).split('\" href=\"/'))):\n",
        "      try:\n",
        "        cust_rev['id'].append(str(saverr['links'][0]).split(';url=%2F')[xc].split('%2F')[2])\n",
        "        cust_rev['name'].append(str(saverr['links'][0]).split(';url=%2F')[xc].split('%2F')[0])\n",
        "      except:\n",
        "        cc = 2\n",
        "\n",
        "    reviews = pd.DataFrame()\n",
        "    reviews['id'] = cust_rev['id']\n",
        "    reviews['name'] = cust_rev['name']\n",
        "\n",
        "    for xi in range(len(reviews)):\n",
        "        cust_rev['link'].append('https://www.amazon.com/'+str(reviews['name'][xi])+'/product-reviews/'+str(reviews['id'][xi])+'/ref=cm_cr_dp_d_show_all_btm?ie=UTF8&reviewerType=all_reviews')\n",
        "    review_links = pd.DataFrame(cust_rev)\n",
        "\n",
        "\n",
        "    for xv in range(len(review_links['link'])):\n",
        "      for xvv in range(level):\n",
        "          xvv = xvv+1\n",
        "          html = requests.get(str(review_links['link'][xv])+'=all_reviews&pageNumber='+str(xvv),headers = headers, cookies =cookies).text\n",
        "          soup = bs4.BeautifulSoup(html, \"html.parser\")\n",
        "          time.sleep(sleep_time)\n",
        "\n",
        "\n",
        "\n",
        "          for tag in soup.findAll(\"span\", {\"class\": \"a-size-base\"}):\n",
        "              xxxxx = tag.findNext().text\n",
        "              if len(xxxxx)>=1:\n",
        "                saver['text'].append(xxxxx)\n",
        "              else:\n",
        "                vvvv = 2\n",
        "\n",
        "          for tag in soup.findAll(\"span\", {\"data-hook\": \"review-body\"}):\n",
        "              xxxxx = tag.findNext().text\n",
        "              if len(xxxxx)>=1:\n",
        "                saver['text'].append(xxxxx)\n",
        "              else:\n",
        "                vvvv = 2\n",
        "\n",
        "          for x in range(len(saver['text'])):\n",
        "            if len(str(saver['text'][x]).split())>=100:\n",
        "              uu['text'].append(str(saver['text'][x]).lower())\n",
        "            else:\n",
        "              ff= 1\n",
        "          for xx in range(len(uu['text'])):\n",
        "            try:\n",
        "              x = str(uu['text'][xx])\n",
        "              cc['text'].append(re.sub('\\n', '', x))\n",
        "            except:\n",
        "              fd = 2\n",
        "          xt = 0\n",
        "          try:\n",
        "            for x in range(len(cc['text'])):\n",
        "              xt = xt+1 \n",
        "              if str(cc['text'][x]).split()[0] == 'edit:':\n",
        "                ttt = 1\n",
        "              elif str(cc['text'][x]).split()[0] == 'edit':\n",
        "                ttt = 1\n",
        "              elif str(cc['text'][x]).split()[1] == 'edit:':\n",
        "                ttt = 1\n",
        "              elif str(cc['text'][x]).split()[1] == 'edit':\n",
        "                ttt = 1\n",
        "              elif str(cc['text'][x]).split()[2] == 'edit:':\n",
        "                ttt = 1\n",
        "              elif str(cc['text'][x]).split()[2] == 'edit':\n",
        "                ttt = 1\n",
        "              elif str(cc['text'][x]).split()[3] == 'edit:':\n",
        "                ttt = 1\n",
        "              elif str(cc['text'][x]).split()[3] == 'edit':\n",
        "                ttt = 1\n",
        "              elif str(cc['text'][x]).split()[4] == 'edit:':\n",
        "                ttt = 1\n",
        "              elif str(cc['text'][x]).split()[4] == 'edit':\n",
        "                ttt = 1\n",
        "              elif str(cc['text'][x]).split()[0] == 'update:':\n",
        "                ttt = 1\n",
        "              elif str(cc['text'][x]).split()[0] == 'update':\n",
        "                ttt = 1\n",
        "              elif str(cc['text'][x]).split()[1] == 'update:':\n",
        "                ttt = 1\n",
        "              elif str(cc['text'][x]).split()[1] == 'update':\n",
        "                ttt = 1\n",
        "              elif str(cc['text'][x]).split()[2] == 'update:':\n",
        "                ttt = 1\n",
        "              elif str(cc['text'][x]).split()[2] == 'update':\n",
        "                ttt = 1\n",
        "              elif str(cc['text'][x]).split()[3] == 'update:':\n",
        "                ttt = 1\n",
        "              elif str(cc['text'][x]).split()[3] == 'update':\n",
        "                ttt = 1\n",
        "              elif str(cc['text'][x]).split()[4] == 'update:':\n",
        "                ttt = 1\n",
        "              elif str(cc['text'][x]).split()[4] == 'update':\n",
        "                ttt = 1\n",
        "\n",
        "              else:\n",
        "                ccc['text'].append(cc['text'][x])\n",
        "          except:\n",
        "            dfs = 1\n",
        "\n",
        "  df_reviews = pd.DataFrame()\n",
        "  df_reviews['reviews'] = pd.DataFrame(ccc['text']).drop_duplicates()\n",
        "else:\n",
        "  print('no data collection')\n",
        "\n",
        "if topicc_model =='yes':\n",
        "  #TOPIC MODEL\n",
        "  import pandas as pd\n",
        "  import re\n",
        "  from gensim import corpora, models, similarities\n",
        "  import nltk\n",
        "  from nltk.corpus import stopwords\n",
        "  import numpy as np\n",
        "  import pandas as pd\n",
        "  import sys\n",
        "  # !{sys.executable} -m spacy download en\n",
        "  import re, numpy as np, pandas as pd\n",
        "  from pprint import pprint\n",
        "\n",
        "  import gensim, spacy, logging, warnings\n",
        "  nlp = spacy.load('en_core_web_sm')\n",
        "  import gensim.corpora as corpora\n",
        "  from gensim.utils import  simple_preprocess\n",
        "  from gensim.models import CoherenceModel\n",
        "  import matplotlib.pyplot as plt\n",
        "\n",
        "  from nltk.corpus import stopwords\n",
        "  stop_words=['from', 'subject', 're', 'edu', 'use', 'not', 'would', 'say', 'could', '_', 'be', 'know', 'good', 'go', 'get', 'do', 'done', 'try', 'many', 'some', 'nice', 'thank', 'think', 'see', 'rather', 'easy', 'easily', 'lot', 'lack', 'make', 'want', 'seem', 'run', 'need', 'even', 'right', 'line', 'even', 'also', 'may', 'take', 'come']\n",
        "\n",
        "  warnings.filterwarnings(\"ignore\",category=DeprecationWarning)\n",
        "  logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.ERROR)\n",
        "  red_it = pd.DataFrame()\n",
        "  red_it['personal_stories'] = df_reviews['reviews']\n",
        "  red_it = pd.DataFrame(red_it['personal_stories'])\n",
        "\n",
        "  #---------------------------------------------------------------------------------\n",
        "\n",
        "  import gensim\n",
        "  from gensim.utils import simple_preprocess\n",
        "  import nltk\n",
        "  nltk.download('stopwords')\n",
        "  from nltk.corpus import stopwords\n",
        "  stop_words = stopwords.words('english')\n",
        "  stop_words.extend(['from', 'subject', 're', 'edu', 'use','www','https','com','fags','string','sleep','apnea','cpap','co','machine','get','getting'])\n",
        "  def sent_to_words(sentences):\n",
        "      for sentence in sentences:\n",
        "          yield(gensim.utils.simple_preprocess(str(sentence), deacc=True))\n",
        "  def remove_stopwords(texts):\n",
        "      return [[word for word in simple_preprocess(str(doc)) \n",
        "                if word not in stop_words] for doc in texts]\n",
        "  data = red_it.personal_stories.values.tolist()\n",
        "  data_words = list(sent_to_words(data))\n",
        "  data_words = remove_stopwords(data_words)\n",
        "\n",
        "  import gensim.corpora as corpora\n",
        "  id2word = corpora.Dictionary(data_words)\n",
        "  texts = data_words\n",
        "  corpus = [id2word.doc2bow(text) for text in texts]\n",
        "  from pprint import pprint\n",
        "  lda_model = gensim.models.LdaMulticore(corpus=corpus,\n",
        "                                          id2word=id2word,\n",
        "                                          num_topics=xcv)\n",
        "  pprint(lda_model.print_topics())\n",
        "  doc_lda = lda_model[corpus]\n",
        "\n",
        "\n",
        "  #---------------------------------------------------------------------------------\n",
        "\n",
        "\n",
        "  def sent_to_words(sentences):\n",
        "      for sent in sentences:\n",
        "          sent = gensim.utils.simple_preprocess(str(sent), deacc=True) \n",
        "          yield(sent)\n",
        "\n",
        "  data = red_it.personal_stories.values.tolist()\n",
        "  data_words = list(sent_to_words(data))\n",
        "  print(data_words[:1])\n",
        "\n",
        "\n",
        "  bigram = gensim.models.Phrases(data_words, min_count=5, threshold=100) # higher threshold fewer phrases.\n",
        "  trigram = gensim.models.Phrases(bigram[data_words], threshold=100)  \n",
        "  bigram_mod = gensim.models.phrases.Phraser(bigram)\n",
        "  trigram_mod = gensim.models.phrases.Phraser(trigram)\n",
        "\n",
        "\n",
        "  def process_words(texts, stop_words=stop_words, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']):\n",
        "      \"\"\"Remove Stopwords, Form Bigrams, Trigrams and Lemmatization\"\"\"\n",
        "      texts = [[word for word in simple_preprocess(str(doc)) if word not in stop_words] for doc in texts]\n",
        "      texts = [bigram_mod[doc] for doc in texts]\n",
        "      texts = [trigram_mod[bigram_mod[doc]] for doc in texts]\n",
        "      texts_out = []\n",
        "      for sent in texts:\n",
        "          doc = nlp(\" \".join(sent)) \n",
        "          texts_out.append([token.lemma_ for token in doc if token.pos_ in allowed_postags])\n",
        "      texts_out = [[word for word in simple_preprocess(str(doc)) if word not in stop_words] for doc in texts_out]    \n",
        "      return texts_out\n",
        "\n",
        "  data_ready = process_words(data_words)  \n",
        "\n",
        "\n",
        "  #---------------------------------------------------------------------------------\n",
        "\n",
        "  id2word = corpora.Dictionary(data_ready)\n",
        "\n",
        "  corpus = [id2word.doc2bow(text) for text in data_ready]\n",
        "\n",
        "  lda_model = gensim.models.ldamodel.LdaModel(corpus=corpus,\n",
        "                                              id2word=id2word,\n",
        "                                              num_topics=xcv, \n",
        "                                              random_state=100,\n",
        "                                              update_every=1,\n",
        "                                              chunksize=10,\n",
        "                                              passes=10,\n",
        "                                              alpha='symmetric',\n",
        "                                              iterations=100,\n",
        "                                              per_word_topics=True)\n",
        "\n",
        "\n",
        "  #---------------------------------------------------------------------------------\n",
        "\n",
        "\n",
        "\n",
        "  def format_topics_sentences(ldamodel=None, corpus=corpus, texts=data):\n",
        "      sent_topics_df = pd.DataFrame()\n",
        "\n",
        "      for i, row_list in enumerate(ldamodel[corpus]):\n",
        "          row = row_list[0] if ldamodel.per_word_topics else row_list            \n",
        "          row = sorted(row, key=lambda x: (x[1]), reverse=True)\n",
        "          for j, (topic_num, prop_topic) in enumerate(row):\n",
        "              if j == 0:\n",
        "                  wp = ldamodel.show_topic(topic_num)\n",
        "                  topic_keywords = \", \".join([word for word, prop in wp])\n",
        "                  sent_topics_df = sent_topics_df.append(pd.Series([int(topic_num), round(prop_topic,4), topic_keywords]), ignore_index=True)\n",
        "              else:\n",
        "                  break\n",
        "      sent_topics_df.columns = ['Dominant_Topic', 'Perc_Contribution', 'Topic_Keywords']\n",
        "\n",
        "      contents = pd.Series(texts)\n",
        "      sent_topics_df = pd.concat([sent_topics_df, contents], axis=1)\n",
        "      return(sent_topics_df)\n",
        "\n",
        "\n",
        "  df_topic_sents_keywords = format_topics_sentences(ldamodel=lda_model, corpus=corpus, texts=data_ready)\n",
        "\n",
        "  df_dominant_topic = df_topic_sents_keywords.reset_index()\n",
        "  df_dominant_topic.columns = ['Document_No', 'Dominant_Topic', 'Topic_Perc_Contrib', 'Keywords', 'Text']\n",
        "  df_dominant_topic.head(10)\n",
        "\n",
        "\n",
        "\n",
        "  pd.options.display.max_colwidth = 100\n",
        "\n",
        "  sent_topics_sorteddf_mallet = pd.DataFrame()\n",
        "  sent_topics_outdf_grpd = df_topic_sents_keywords.groupby('Dominant_Topic')\n",
        "\n",
        "  for i, grp in sent_topics_outdf_grpd:\n",
        "      sent_topics_sorteddf_mallet = pd.concat([sent_topics_sorteddf_mallet, \n",
        "                                                grp.sort_values(['Perc_Contribution'], ascending=False).head(1)], \n",
        "                                              axis=0)\n",
        "\n",
        "  sent_topics_sorteddf_mallet.reset_index(drop=True, inplace=True)\n",
        "\n",
        "  sent_topics_sorteddf_mallet.columns = ['Topic_Num', \"Topic_Perc_Contrib\", \"Keywords\", \"Representative Text\"]\n",
        "\n",
        "  sent_topics_sorteddf_mallet.head(10)\n",
        "\n",
        "  books_read = sent_topics_sorteddf_mallet\n",
        "\n",
        "  #---------------------------------------------------------------------------------\n",
        "\n",
        "  from matplotlib import pyplot as plt\n",
        "  from wordcloud import WordCloud, STOPWORDS\n",
        "  import matplotlib.colors as mcolors\n",
        "\n",
        "  cols = [color for name, color in mcolors.TABLEAU_COLORS.items()]  # more colors: 'mcolors.XKCD_COLORS'\n",
        "\n",
        "  cloud = WordCloud(stopwords=stop_words,\n",
        "                    background_color='white',\n",
        "                    width=2500,\n",
        "                    height=1800,\n",
        "                    max_words=20,\n",
        "                    colormap='tab10',\n",
        "                    color_func=lambda *args, **kwargs: cols[i],\n",
        "                    prefer_horizontal=1.0)\n",
        "\n",
        "  topics = lda_model.show_topics(formatted=False)\n",
        "\n",
        "  fig, axes = plt.subplots(4, 4, figsize=(20,20), sharex=True, sharey=True)\n",
        "  try:\n",
        "    for i, ax in enumerate(axes.flatten()):\n",
        "      fig.add_subplot(ax)\n",
        "      topic_words = dict(topics[i][1])\n",
        "      cloud.generate_from_frequencies(topic_words, max_font_size=300)\n",
        "      plt.gca().imshow(cloud)\n",
        "      plt.gca().set_title('Topic ' + str(i), fontdict=dict(size=16))\n",
        "      plt.gca().axis('off')\n",
        "  except:\n",
        "      dffd = 1\n",
        "\n",
        "  try:\n",
        "    plt.subplots_adjust(wspace=0, hspace=0)\n",
        "    plt.axis('off')\n",
        "    plt.margins(x=0, y=0)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "  except:\n",
        "    dffd = 1\n",
        "\n",
        "  #---------------------------------------------------------------------------------\n",
        "\n",
        "  from collections import Counter\n",
        "  topics = lda_model.show_topics(formatted=False)\n",
        "  data_flat = [w for w_list in data_ready for w in w_list]\n",
        "  counter = Counter(data_flat)\n",
        "\n",
        "  out = []\n",
        "  for i, topic in topics:\n",
        "      for word, weight in topic:\n",
        "          out.append([word, i , weight, counter[word]])\n",
        "\n",
        "  df = pd.DataFrame(out, columns=['word', 'topic_id', 'importance', 'word_count'])        \n",
        "\n",
        "  fig, axes = plt.subplots(4, 4, figsize=(20,20), sharey=True, dpi=160)\n",
        "  cols = [color for name, color in mcolors.TABLEAU_COLORS.items()]\n",
        "  try:\n",
        "    for i, ax in enumerate(axes.flatten()):\n",
        "        ax.bar(x='word', height=\"word_count\", data=df.loc[df.topic_id==i, :], color=cols[i], width=0.5, alpha=0.3, label='Word Count')\n",
        "        ax_twin = ax.twinx()\n",
        "        ax_twin.bar(x='word', height=\"importance\", data=df.loc[df.topic_id==i, :], color=cols[i], width=0.2, label='Weights')\n",
        "        ax.set_ylabel('Word Count', color=cols[i])\n",
        "        ax_twin.set_ylim(0, 0.030); ax.set_ylim(0, 3500)\n",
        "        ax.set_title('Topic: ' + str(i), color=cols[i], fontsize=16)\n",
        "        ax.tick_params(axis='y', left=False)\n",
        "        ax.set_xticklabels(df.loc[df.topic_id==i, 'word'], rotation=30, horizontalalignment= 'right')\n",
        "        ax.legend(loc='upper left'); ax_twin.legend(loc='upper right')\n",
        "\n",
        "    fig.tight_layout(w_pad=2)    \n",
        "    fig.suptitle('Word Count and Importance of Topic Keywords', fontsize=22, y=1.05)   \n",
        "    plt.show()\n",
        "  except:\n",
        "    fwf = 1\n",
        "else:\n",
        "  print('no topic model')\n",
        "\n",
        "df_reviews\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LIoHvACbtGZi",
        "outputId": "8f8d1953-efba-4925-b6f9-d3aab2daabe5"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "run data collection? (yes/no)\n",
            "yes\n",
            "run topic model? (yes/no)\n",
            "yes\n",
            "enter your level (1-10)\n",
            "1\n",
            "enter your preferred sleep time (for levels > 10, over 5s is reccommended):\n",
            "0\n",
            "enter your preferred # of topics for the topic model:\n",
            "20\n",
            "enter the product type / brand reviews you would like to analyze\n",
            "air purifier\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ccc['text']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kvHjGsSqzcMl",
        "outputId": "bd762ee4-e7e3-4318-9bc8-8039381448ff"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[]"
            ]
          },
          "metadata": {},
          "execution_count": 449
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "  df_reviews = pd.DataFrame()\n",
        "  df_reviews['reviews'] = pd.DataFrame(ccc['text']).drop_duplicates()\n",
        "  len(df_reviews)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 201
        },
        "id": "PPW4oQXUxk_e",
        "outputId": "ae9cc00d-577b-4b7d-d340-dac1323221da"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-eaf1f3e4e265>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf_reviews\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mdf_reviews\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'reviews'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mccc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'text'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop_duplicates\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_reviews\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'pd' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n"
      ],
      "metadata": {
        "id": "nZrnV3V2h6hz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "RxO9CQ4Y0i3L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "IaoTscvQ0i9S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "xPzy1VqV0jAl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "9y3TpWt70jDp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "eE7QihtP0jK-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "t89sY2jU0jNv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "xutuMccn0jQr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#scrapyard"
      ],
      "metadata": {
        "id": "C-TdliCw0jUX"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "name": "Copy of Copy of All_Hot_Prods.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}